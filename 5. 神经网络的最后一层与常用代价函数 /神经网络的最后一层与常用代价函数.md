我们知道ANN最后的输出可以作为分类或者回归问题的预测结果。

对于回归问题，输出层可以使用线性激活函数来输出连续的实数，并使用**均方误差代价函数**来衡量代价，这样在输出层可以避免梯度消失的问题。

对于分类问题，我们的输出层往往需要使用一些特殊的激活函数，比如使用sigmoid函数或者softmax函数来将输入z值转换为概率值输出，其中sigmoid函数可以用来做二分类任务的ANN输出层的激活函数，softmax函数可以用来做多分类（包括二分类）任务的ANN输出层的激活函数。这时候我们可以看到sigmoid函数做输出层激活函数并使用均方误差代价函数之后，会使得反向传播算法从输出层开始就会产生残差衰减。

这时候我们就需要考虑如何避免残差衰减？可以看到对残差衰减看到起到决定性作用的就是sigmoid函数的导数，我们在使用均方误差代价函数之后，从输出层开始每一层都会乘以一个≤0.25的数值。如何避免引入sigmoid函数的导数呢？在输出层我们可以改变代价函数，在隐藏层我们可以更换激活函数。后者我们在以后讨论。对于前者我们确实可以找到更好的代价函数，下面我们就讲在分类任务中两种常用的代价函数：**交叉熵代价函数(Cross-entropy cost function)**和**对数似然代价函数(Log-likelihood cost function)**

# sigmoid输出层与交叉熵

由于sigmoid函数的定义域为实数集，值域为(0, 1)，并以0.5为界限，是很好的二分类机。如果我们把sigmoid函数作为输出层（此处输出层仅有一个神经元）的激活函数，那么ANN就可以用来做二分类。

### sigmoid函数

$$
h=\frac{1}{1+e^{-z}}
$$

*注：在神经网络中我们使用z来表示层输入，x表示样本输入，为了下面式子的方便，我们此处的函数使用的自变量是z（下面的图像为了绘图方便使用的是x）。*

sigmoid函数图像如下：

<img src="../2. 构造神经网络解决简单逻辑运算/sigmoid.png" width="400px">

sigmoid函数可以看做是将输入x变换为*输出是正类的相对可能性*。

如果我们用h表示正类(1-h)表示父类，则**几率(Odds)**的定义为h/(1-h)。对几率取对数可以得到**对数几率(Logit odds)**为ln(h/(1-h))，对数几率有很好的数学特性，它是任意阶可导的凸函数。我们把sigmoid函数表达式带入对数几率式子可得：
$$
\ln\frac{h}{1-h}=z
$$
那么当选用sigmoid层为输出层之后，选用那个代价函数呢？

### 交叉熵代价函数

$$
J=-\frac{1}{m}\sum_{i=1}^m[y_i\ln{h(x_i)}+(1-y_i)\ln(1-h(x_i))]
$$

在二分类任务中标记y∈{0, 1}。交叉熵可以用来衡量预测值与标记值的误差，误差越大，代价越大。

抛开熵的概念，我们依然可以通过**极大似然法(Maximum likelihood method)**得到sigmoid输出层的代价函数。

推导过程如下：

若将标记y视为类后验概率估计，则有:
$$
p(y=1|x)=h
$$

$$
p(y=0|x)=1-h
$$

对给定数据集对模型取**对数似然(Log-likelihood)**可得：
$$
l=\sum_{i=1}^m\ln{p(y_i|x_i;w,b)}
$$
根据类后验概率估计的式子即可以得到：
$$
p(y_i|x_i;w,b)=y_{i}p(y=1|x_i)+(1-y_i)p(y=0|x_i)
$$
将上式带入对数似然函数，并根据类后验概率估计可得：
$$
l=\sum_{i=1}^{m}(y_ih(x_i)+(1-y_i)(1-h(x_i)))
$$
我们的目标是最大化平均对数似然，也就是最小化负的平均对数似然，即：
$$
min(-\hat{l})=-\frac{1}{m}\sum_{i=1}^m[y_i\ln{h(x_i)}+(1-y_i)\ln(1-h(x_i))]=J
$$
到此我们推出了其代价函数，可以看到在形式上与交叉熵代价函数一致。

### 交叉熵代价函数的梯度下降

我们仍然以单样本为例，则代价函数变为：
$$
J=-y\ln{h(x)}+(y-1)\ln{(1-h(x))}
$$
输出层的残差表示为：
$$
\delta^{(L)}=\frac{\partial{J}}{\partial{z^{(L)}}}
=\frac{\partial{J}}{\partial{a^{(L)}}}\frac{\partial{a^{(L)}}}{\partial{z^{(L)}}}
=\sigma(z^{(L)})-y
$$
输出层的连接权重的梯度：
$$
\frac{\partial{J}}{\partial{w^{(L)}_{jk}}}=\frac{\partial{J}}{\partial{z_j^{(L)}}}\frac{\partial{z_j^{(L)}}}{\partial{w^{(L)}_{jk}}}
\\=(\sigma{(z_j^{(L)})}-y)·\frac{\partial}{\partial{w_{jk}^{(L)}}}(\sum_{K=1}^{N^{L-1}}(w_{jK}^{L}a_K^{(L-1)}+b_j^{(L)}))
\\=(\sigma{(z_j^{(L)})}-y)a_k^{(L-1)}
$$
同理，输出层偏置的梯度：
$$
\frac{\partial{J}}{\partial{b^{(L)}_j}}=\sigma{(z_j^{(L)})}-y
$$
可以看到使用交叉熵代价函数，在反向传播算法中输出层的连接权重与偏置值的梯度均不含sigmoid函数的导数，所以输出层可以避免因sigmoid函数的导数带来的梯度消失。

# Softmax层与交叉熵代价函数

二分类任务中，可以使用sigmoid函数对z进行变换，在多分类任务中显然再使用sigmoid函数就不合适了。所以我们引入了softmax函数作为输出层的激活函数。softmax函数如其名字一样，可以对多个输入z进行排序（此处需要注意sigmoid函数作为输出层是只有一个神经元，也就是在输出层只有一个输入z和一个输出h，而softmax则是有与标记的分类数目一样多的神经元。）。并将输入z按照输入大小转化为对应的概率（输入越大，概率越大）。softmax与max函数相比最大的特点在于其既可以输出概率最大值，又可以避免概率小的值不至于取不到。

softmax函数的定义如下：
$$
h_i=\frac{e^{z_i}}{\sum_{j}e^{z_j}}
$$
可以看到softmax函数做的事情很简单，就是将任意的输入x转化为概率分布。exp(xi)可以保证概率为正数，再通过归一化使所有概率之和为1。

### 对数似然代价函数

与上述的sigmoid函数利用极大似然估计法得到代价函数一样，此处的推导过程与上述一致，我们在此直接得到代价函数：
$$
J=-\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{N^{(L)}}y_j^{(i)}\ln{h_j^{(i)}(x)}
$$

*注意：此处的带括号的上标i表示的是样本索引，上标L表示层索引，我们在推导公式的时候是以单个样本为例的，推导过程中的带括号的上标均表示层索引。*

### 对数似然函数的梯度下降

我们首先将代价函数转化为单样本的代价函数：
$$
J=-\sum_{j=1}^{N^{(L)}}y_j\ln{h_j(x)}
$$
输出层的残差表示为：
$$
\delta^{(L)}_j=\frac{\partial{J}}{\partial{z_j^{(L)}}}
=\frac{\partial{J}}{\partial{a_j^{(L)}}}\frac{\partial{a_j^{(L)}}}{\partial{z_j^{(L)}}}
=-y_j·\frac{1}{a_j^{(L)}}·\frac{\partial{J}}{\partial{z_j^{(L)}}}
$$
输出层连接权重的梯度为：
$$
\frac{\partial{J}}{\partial{w_{jk}^{(L)}}}=\frac{\partial{J}}{\partial{z_j^{(L)}}}\frac{\partial{z_{jk}^{(L)}}}{\partial{w_{jk}^{(L)}}}
=\delta_j^{(L)}·\frac{\partial}{\partial{w_{jk}^{(L)}}}\sum_{K=1}^{N^{L-1}}(w_{jK}^{(L)}a_K^{(L-1)}+b_j^{(L)})
=\delta_j^{(L)}·a_k^{(L-1)}
$$
同理可得输出偏置的梯度：
$$
\frac{\partial{J}}{\partial{b^{(L)}_j}}=\delta_j^{(L)}
$$



