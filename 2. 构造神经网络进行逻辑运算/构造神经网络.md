# 构造一个神经元

> 生物神经元具有简单的结构，神经元与神经元互联构成了复杂的神经网络。本节从分析生物神经元开始，然后用数学的语言描述其特性并构建一个神经元。

### 生物意义上的神经元

生物神经元由树突输入信号(信号来源于上一个神经元细胞或感受器)，进入胞体存留，达到阈值时，信号通过轴突传导出去(给下一个神经元或肌肉或腺体)。神经元具有**兴奋性**和**传导性**。这里

1. 树突存在多个，每个均可以传导电流进入胞体并存储。

2. 树突与上一个信号源的连接强弱程度不同，连接强度大则传导的电流大。

3. 胞体可以将每个树突传导来的电流存储在一起。

4. 每个神经元的胞体可以存储的电流多少不同，既触发传导下去的阈值不同。

5. 胞体传导出的电流强弱不一定与存储的电流相同。

6. 轴突传导出去的电流可以被多个树突检测到。

   ![neuron](./images/neuron.png)

   ​

### 数学表示

用**输入**来表示突触接收的电流，那么输入可以表示为：
$$
s = ∑Wi * Xi
$$
其中 **X** 表示传导来的信号的向量，**W** 表示与上一个信号源的连接强弱系数，**i** 表示第i个突触，**s** 表示电流总和。

用[**激活函数 f(·) **](https://en.wikipedia.org/wiki/Activation_function)来表示胞体的特性：兴奋强度达到阈值传导兴奋，否则不传导。

![neuron_model](./images/neuron_model.jpeg)

激活函数包括(**s**表示电流总和即激活函数输入，**a**表示激活函数输出):

1. 阶梯函数(Step)：当 s <= 0, a = 0; 当 s > 0, a = 1 。
2. 符号函数(Sgn)：当 s < 0, a = -1; 当 s >= 0, a = 1 。
3. 线性函数(Linear)：a = s 。
4. 饱和线性函数(Ramp)：当 s < 0, a = 0; 当 0 <= s <= 1, a = s; 当 s > 1, a = 1 。
5. **对数S形函数(Sigmoid)：a = 1 / ( 1 + exp(-s) )  。**
6. **双曲正切S形函数(Tanh): a =( exp(s) - exp(-s) ) / ( exp(s) + exp(-s) ) 。**
7. SoftPlus激活函数：a = log( 1 + exp(s) ) 。
8. **强制非负校正函数(ReLU)：a = max( 0, s ) 。**
9. Leaky-ReLU(LReLU)函数：当 s < 0, a = q * s, 当 s >= 0, a = s 。(q是一个很小的常数)
10. **带参ReLU(PReLU)：与LReLU相比，q是一个可变参数，通过反向传播进行跟新。**
11. Maxout：max(Zij), Zij = Sij, maxout激活函数实际相当于是一个激活函数与输入层之间的小规模的隐藏层。
12. …...

用**输出**表示神经元的输出，那么激活函数的输出即为输出。

注：如果以sigmoid作为神经元的激活函数，则此神经元与逻辑斯蒂回归的表达式是一样的。

### Python Code 

```python
import numpy as np

class Neuron(object):
    '''sigmoid神经元'''
    def __init__(self):
        self.weights = 0
        self.bias = 0
        self.inputs = 0
        self.output = 0
        self.len_inputs = -1
    
    def sigmoid(self, x):
        '''sigmoid实现 f = 1/(1 + e^(-x))'''
        return 1/(1 + np.exp(-x))
    
    def forward(self, inputs):
        '''利用inputs构造神经元，输出激活值。
        
        Args:
        	inputs: shape=(n, ) n表示输入的参数数量
        Return:
        	一个常量
        '''
        self.inputs = inputs
        if self.len_inputs == -1:
            # 使用均值为0，方差为0.1的值来初始化权重
            self.len_inputs = inputs.shape[0]
            self.weights = np.random.normal(
                loc=.0, scale=.1, size=self.len_inputs)
        self.output = self.sigmoid(np.dot(self.weights.T, self.inputs) + self.bias)
        return self.output
```

到此，已经仿造生物神经元构造完成一个神经元，接下来，需要构造神经网络。

# 构造一个神经网络(ANN)

> 本节会利用上面生成的神经元构造人工神经网络。

生物上的神经网络由很多个神经元构成，同时神经元之间的连接往往表现为分层结构。也就是说：很多神经元构成一个神经网络层，很多神经网络层构成一个神经网络。

根据生物上的神经网络结构，可以用有向无环图建模。具体为一些神经元(第n层)的输出可以变为其它神经元(第n+1层)的输入。其中最常见的是全连接神经网络。

全连接神经网络是由多个全连接层组成的。

下面是一个全连接神经网络的网络拓扑结构：![neural_net](./images/neural_net.jpeg)

此神经网络有四层，其中第一层为**输入层**，最后一层为**输出层**，中间的层为**隐藏层**。输入层有3个神经元，第一、二个隐藏层均有4个神经元，输出层有1个神经元。**同一层内的各个神经元是互不相连的，相邻层之间的神经元是全连接的。**

注：回归或支持向量机仅仅是单层神经网络的一种特殊情况。

注：神经网络(ANN)也被叫做**多层感知机(MLP)**

神经网络规模的衡量方法有两种：① 神经元数量与神经网络层数。② 参数数量。上面展示的神经网络共有4层，3+4+4+1=12个神经元；共有(3\*4+4) + (4\*4+4) + (4*1+1) = 41个参数。

### Python Code 

```python
class Layer(object):
    '''实现神经网络的一个层'''
    def __init__(self, num_node):
        '''输入当前层的神经元个数，构造神经网络的一个层。'''
        self.num_node = num_node
        self.neurons = [Neuron() for _ in range(num_node)]
        self.inputs = 0
        self.output = 0
    
    def calc(self, x):
        self.inputs = x
        self.output = np.array([n.forward(x) for n in self.neurons])
        return self.output
    
    
class ANNNet(object):
    def __init__(self, layers_detail=[4, 4, 1]):
        self.layers_detail = layers_detail
        self.layers = [Layer(n) for n in self.layers_detail]
        
    def fit(self, inputs):
        output = inputs
        for layer in self.layers:
            output = layer.calc(output)
        return output
```

# Sigmoid激活函数说明

神经网络中的激活函数通常均为非线性的。sigmoid函数是常用的激活函数之一。sigmoid函数接收任意输入，但是输出值的范围是(0, 1)，将sigmoid函数的值作为输出可以当做概率进行使用。

sigmoid激活函数的优点：

1. 非线性。
2. 输出范围有限，数据传递过程中不会出现发散的情况。
3. 输出值可以直接作为概率进行使用。
4. 求导容易。

sigmoid激活函数的缺点：

1. 饱和的时候梯度很小，下架速度近乎为0。
2. 梯度下降过程中容易出现波动。

**sigmoid图像：**

![sigmoid](./images/sigmoid.png)

关于sigmoid等激活函数，在之后，我们会更加详细的讲解，在此作为一个了解。