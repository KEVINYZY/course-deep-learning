**反向传播算法(Back propagation, BP)**是目前训练ANN最常用最有效的算法。主要思想是利用ANN的输出结果与实际结果（标记）的误差从输出层向隐藏层反向传播，直至传播到第二层（即除了输入层）为止。在反向传播的过程中，根据误差调整参数的值；不断迭代，直至收敛。

本节的主要内容是反向传播算法的公式推导。我们继续使用均方误差代价函数来衡量误差：
$$
J=\frac{1}{2m}\sum_{i=1}^{m}||y^{(i)}-h^{(i)}_{w,b}(x)||^2
$$

### 变量定义

在推导公式之前，我们定义一些变量，这些变量可以帮助我们简化推导公式，使之更容易理解。

<img src="./images/ann.jpg" width="400px">

上图是一个三层ANN，layer1为输入层，layer2为隐藏层，layer3为输出层。
$$
l表示层索引，L表示最后一层的索引。j、k均表示某一层神经元的索引。\\N表示神经网络每一层的神经元数量。w表示连接权重，b表示偏置值。
$$

$$
w_{jk}^{(l)}表示第l层的第j个神经元与第(l-1)层的第k个神经元的连接权重。
$$

$$
b_j^{(l)}表示第l层的第j个神经元的偏置值。
$$

$$
{N^{(l)}}表示第l层神经元的数量。其中N^{(L)}表示最后一层神经元的数量。
$$

$$
z_j^{(l)}=\sum_{k=1}^{N^{(l-1)}}w^{(l)}_{jk}a_k^{(l-1)}+b_j^{(l)}表示第l层第j个神经元的输入。
$$

$$
a_j^l=\sigma({z_j^{(l)}})表示第l层第j个神经元的输出。其中\sigma表示sigmoid激活函数。
$$

### 公式推导

首先，将第l层第j个神经元产生的残差定义为：
$$
\delta_j^{(l)}=\frac{\partial{J}}{\partial{z_j^{(l)}}}
$$
为避免多个样本所带来的上标复杂难记，我们以单样本为例，此时的代价函数为：
$$
J=\frac{1}{2}||y-h(x)||^2=\frac{1}{2}||y-a^{(L)}||^2=\frac{1}{2}\sum_{j=1}^{N^{(L)}}(y_j-a_j^{(L)})^2
$$
*注意：这里所说的“残差”表示“输入错误”或“对输入求导”。计算残差的意义在于决定每一层神经元的状态的参数w，b均在输入z中。在深度学习中，还有一个地方会提到“残差”——残差神经网络。残差神经网络的残差与此处的残差并不是一种概念。*

##### 输出层残差：#####

$$
\delta^{(L)}=\nabla_aJ\odot{\sigma^{'}(z^{(L)})}
$$

推导过程如下：
$$
\because\delta_j^{(L)}=\frac{\partial{J}}{\partial{a_j^{(L)}}}·\frac{\partial{a_j^{(L)}}}{\partial{z_j^{(L)}}}\\\therefore\delta^{(L)}=\frac{\partial{J}}{\partial{a^{(L)}}}\odot\frac{\partial{a^{(L)}}}{\partial{z^{(L)}}}\\\therefore=\nabla_aJ\odot{\sigma{'}(z^{(L)})}
$$

##### 隐藏层残差：#####

$$
\delta^{(l)}=((w^{(l+1)})^T\delta^{(l+1)})\odot\sigma'(z^{(l)})
$$

推导过程如下：
$$
\because\delta_j^{(l)}=\frac{\partial{J}}{\partial{z^{(l)}_j}}=\sum_{k=1}^{N^{(l+1)}}\frac{\partial{J}}{\partial{z_k^{(l+1)}}}·\frac{\partial{z_k^{(l+1)}}}{\partial{a^{(l)}_j}}·\frac{\partial{a^{(l)}_j}}{\partial{z_j^{(l)}}}\\=\sum_{k=1}^{N^{(l+1)}}\delta_k^{(l+1)}·\frac{\partial(w_{kj}^{(l+1)}a_j^{(l)}+b_k^{(l+1)})}{\partial{a_j^{(l)}}}·\sigma'(z_j^{(l)})
\\=\sum_{k=1}^{N^{(L+1)}}\delta_k^{(l+1)}·w_{kj}^{(l+1)}·\sigma'(z_j^{(l)})
$$

$$
\therefore\delta^{(l)}=((w^{(l+1)})^T\delta^{(l+1)})\odot\sigma'(z_j^{(l)})
$$

##### 连接权重的梯度：#####

$$
\frac{\partial{J}}{\partial{w_{jk}^{(l)}}}=a_k^{(l-1)}\delta_j^{(l)}
$$

推导过程如下：
$$
\frac{\partial{J}}{\partial{w_{jk}^{(l)}}}=\frac{\partial{J}}{\partial{z_j^{(l)}}}·\frac{\partial{z_j^{(l)}}}{\partial{w_{jk}^{(l)}}}\\=\delta_j^{(l)}·\frac{\partial(w_{jk}^{(l)}a_k^{(l-1)}+b_j^{(l)})}{\partial{w_{jk}^{(l)}}}\\
=a_k^{(l-1)}\delta_j^{(l)}
$$

##### 偏置梯度#####

$$
\frac{\partial{J}}{\partial{b_j^{(l)}}}=\delta_j^{(l)}
$$

推导过程如下：
$$
\frac{\partial{J}}{\partial{b_j^{(l)}}}=\frac{\partial{J}}{\partial{z_j^{(l)}}}·\frac{\partial{z_j^{(l)}}}{\partial{b_j^{(l)}}}\\=\delta_j^{(l)}·\frac{\partial{(w_{jk}^{(l)}a_k^{(l-1)}+b_j^{(l)})}}{\partial{b_j^{(l)}}}\\=\delta_j^{(l)}
$$
